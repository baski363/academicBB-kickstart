@incollection{BBFM2011EORMS,
 abstract = {The gradient method , which is also called the method of steepest descent, and the Cauchy method, is one of the most fundamental derivative‚Äêbased procedure for unconstrained minimization of a differentiable function. The performance of the method in terms of speed of convergence is lacking, and it tends to suffer from very slow convergence, especially as a stationary point is approached. However, it does guarantee global convergence under reasonable conditions and admits a thorough mathematical analysis of its behavior. For this reason, the gradient method has been used as a starting point in the development of more sophisticated, globally convergent algorithms with better convergence properties for unconstrained minimization. This article presents a cogent overview of this fundamental method and its convergence properties under various settings.},
 author = {Foad Mahdavi Pajouh and Balabhaskar Balasundaram},
 bdsk-url-1 = {http://dx.doi.org/10.1002/9780470400531.eorms0363},
 booktitle = {Wiley Encyclopedia of Operations Research and Management Science},
 date-modified = {2020-08-10 16:15:55 -0500},
 doi = {10.1002/9780470400531.eorms0363},
 editor = {Cochran, J. J. and Cox, L. A. and Keskinocak, P. and Kharoufeh, J. P. and Smith, J. C.},
 isbn = {9780470400531},
 keywords = {unconstrained optimization, gradient method, method of steepest descent, Cauchy method, subgradient method},
 pages = {2092--2099},
 publisher = {John Wiley & Sons, Inc.},
 title = {Gradient-Type Methods},
 url = {http://dx.doi.org/10.1002/9780470400531.eorms0363},
 volume = {3},
 year = {2011}
}

